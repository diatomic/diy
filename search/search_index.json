{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DIY","text":"<p>DIY is a block-parallel library for writing scalable distributed- and shared-memory parallel algorithms that can run both in- and out-of-core. The same program can be executed with one or more threads per MPI process and with one or more data blocks resident in main memory.  The abstraction enabling these capabilities is block-parallelism; blocks and their message queues are mapped onto processing elements (MPI processes or threads) and are migrated between memory and storage by the DIY runtime. Complex communication patterns, including neighbor exchange, merge reduction, swap reduction, and all-to-all exchange, are implemented in DIY.</p> <p>DIY follows a bulk-synchronous processing (BSP) programming model.</p>"},{"location":"#example","title":"Example","text":"<p>A simple program snippet, shown below, consists of the following components:</p> <ul> <li>a <code>struct</code> called <code>Block</code>,<ul> <li>contains all user-defined state related to a block</li> </ul> </li> <li>the <code>diy::Master</code> object called <code>master</code>,<ul> <li><code>master</code> owns and manages the blocks assigned to the current MPI process.</li> </ul> </li> <li>user-defined callback functions performed on each block by <code>master.foreach()</code><ul> <li>in this example <code>enqueue_local()</code> and <code>average()</code></li> </ul> </li> <li>message exchanges between the blocks by <code>master.exchange()</code><ul> <li>data are enqueued and dequeued to/from message queues during <code>master.foreach()</code> callback functions.</li> </ul> </li> </ul> <pre><code>    // --- main program --- //\nstruct Block { float local, average; };             // define your block structure\nMaster master(world);                               // world = MPI_Comm\n...                                                 // populate master with blocks\nmaster.foreach(&amp;enqueue_local);                     // call enqueue_local() for each block\nmaster.exchange();                                  // exchange enqueued data between blocks\nmaster.foreach(&amp;average);                           // call average() for each block\n// --- callback functions --- //\n// enqueue block data prior to exchanging it\nvoid enqueue_local(Block* b,                        // current block\nconst Proxy&amp; cp)                 // communication proxy provides access to the neighbor blocks\n{\nfor (size_t i = 0; i &lt; cp.link()-&gt;size(); i++)  // for all neighbor blocks\ncp.enqueue(cp.link()-&gt;target(i), b-&gt;local); // enqueue the data to be sent to this neighbor\n// block in the next exchange\n}\n// use the received data after exchanging it, in this case compute its average\nvoid average(Block* b,                              // current block\nconst Proxy&amp; cp)                       // communication proxy provides access to the neighbor blocks\n{\nfloat x, average = 0;\nfor (size_t i = 0; i &lt; cp.link()-&gt;size(); i++)  // for all neighbor blocks\n{\ncp.dequeue(cp.link()-&gt;target(i).gid, x);    // dequeue the data received from this\n// neighbor block in the last exchange\naverage += x;\n}\nb-&gt;average = average / cp.link()-&gt;size();\n}\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>More information about getting started creating blocks, assigning them to processes, and decomposing a domain can be found in the Initialization page. Complete examples of working programs can be found in the Examples page.</p>"},{"location":"#topics","title":"Topics","text":"<ul> <li>Initialization</li> <li>Domain decomposition</li> <li>Callback functions</li> <li>Communication<ul> <li>Local communication</li> <li>Remote communication</li> <li>Global commrnication (reductions)</li> </ul> </li> <li>Block I/O</li> <li>Serialization</li> <li>MPI convenience wrapper</li> <li>Algorithms</li> <li>Writing out-of-core algorithms</li> <li>Automatic block threading</li> <li>Examples</li> </ul>"},{"location":"#download","title":"Download","text":"<p>DIY is available on Github, subject to a variation of a 3-clause BSD license.  You can download the latest tarball.</p> <p>Authors Dmitriy Morozov and Tom Peterka</p>"},{"location":"algorithms/","title":"Algorithms","text":"<p>The following algorithms are included in DIY:</p>"},{"location":"algorithms/#sort","title":"Sort","text":"<p>A parallel sample sort that sorts values of each block and computes the boundaries in <code>samples</code>. Optionally, the results can be all_to_all distributed to everyone. A shorter version of the API with a default comparison function and the all_to_all exchange mandatory is also included. The sort uses the same k-ary global reduction as in the communication module.</p> <p>See the example here.</p>"},{"location":"algorithms/#kd-tree","title":"Kd-tree","text":"<p>A set of points initially distributed in a set of blocks can be redistributed in a kd-tree containing the same number of blocks. The number of blocks must be a power of 2, and the depth of the kd-tree will be log_2(num_blocks). The block boundaries are computed using a histogram; hence they are approximate to the bin width of the histogram. The number of histogram bins is user-supplied.</p> <p>See the example here.</p>"},{"location":"async_comm/","title":"Asynchronous communication","text":""},{"location":"async_comm/#asynchronous-iexchange","title":"Asynchronous <code>iexchange</code>","text":"<p>Rather than synchronizing between computations and message exchanges as in local synchronous communication, <code>diy::Master::iexchange</code> attempts to interleave computation and communication and execute the three steps (enqueue, exchange, dequeue) asynchronously and repeatedly until there is no more work left to do and no more messages left in the system. This protocol is for iterative computations that eventually converge, where the result is independent of the order of message arrival and does not require strict synchronization between iterations. While this may seem restrictive, many algorithms, e.g., parallel particle tracing, distributed union-find, and many graph algorithms qualify. Moreover, the global amount of work does not need to monotonically decrease over the execution of the algorithm; new work can suddenly appear, and <code>iexchange</code> will continue to run until the system state is quiet, automatically determining when to terminate. This is exceptionally useful for data-driven, irregular, iterative algorithms. In these cases, the performance of <code>iexchange</code> is often better than <code>exchange</code>, and it can be easier to use.</p> <p>There is only one callback function required, and it is passed directly to <code>iexchange</code>. The body of the callback function includes both dequeuing and enqueuing. DIY handles the messaging internally as it calls this function repeatedly until the function returns that its local work is done, and DIY confirms that there are no pending messages in flight.</p> <p>A remote flag set to true can also be optionally passed to <code>iexchange</code>, combining the features of remote communication with <code>iexchange</code>.</p> <pre><code>// callback for asynchronous iexchange\n// return: true = I'm done unless more work arrives; false = I'm not done, please call me again\nbool foo(Block* b,                             // local block\nconst diy::Master::ProxyWithLink&amp; cp) // communication proxy for neighbor blocks\n{\ndiy::Link*    l = cp.link();               // link to the neighbor blocks\n// compute some local value\n...\n// for all neighbor blocks, enqueue data going to this neighbor block\nfor (int i = 0; i &lt; l-&gt;size(); ++i)\ncp.enqueue(l-&gt;target(i), value);\n// for all neighbor blocks, dequeue data received from this neighbor block\nfor (int i = 0; i &lt; l.size(); ++i)\n{\nint v;\nwhile (cp.incomingl(l-&gt;target(i).gid))   // get everything available\ncp.dequeue(l-&gt;target(i).gid, v);\n}\n// compute some local value\n...\n// determime whether this block is done for now\nbool done = ...;\nreturn done;\n}\nint main(int argc, char**argv)\n{\n...\nmaster.iexchange(&amp;foo);\n}\n</code></pre> <p>A complete example is here</p>"},{"location":"callbacks/","title":"Callback functions","text":"<p>Operations performed on blocks are done by passing callback functions to <code>diy::Master::foreach</code></p>"},{"location":"callbacks/#example","title":"Example","text":"<p>Assume the block is defined as:</p> <pre><code>struct Block\n{\nstd::vector&lt;int&gt;      values;\nfloat                 average;\n};\n</code></pre> <p>An example of a callback function that sums the values in the local block and then enqueues the sum to its neighbors is below.  The communication proxy <code>diy::Master::ProxyWithLink</code> is a local \"communicator\" to the neighboring blocks. This communication proxy is the result of blocks with the same associated link having been previously added to the <code>Master</code>, typically by the <code>Decomposer</code>. From the communication proxy, we can extract the <code>diy::Link</code>, which is the actual communication subgraph linking the neighbors. By iterating over the link, we can enqueue data to particular neighbors. Note that enqueueing only copies data to the message queue destined for the target block; no data are actually sent yet.</p> <pre><code>// compute sum of local values and enqueue the sum\nvoid local_sum(Block* b,                             // local block\nconst diy::Master::ProxyWithLink&amp; cp) // communication proxy for neighbor blocks\n{\ndiy::Link*    l = cp.link();                     // link to the neighbor blocks\n// compute local sum\nint total = 0;\nfor (unsigned i = 0; i &lt; b-&gt;values.size(); ++i)\ntotal += b-&gt;values[i];\n// for all neighbor blocks, enqueue data going to this neighbor block in the next exchange\nfor (int i = 0; i &lt; l-&gt;size(); ++i)\ncp.enqueue(l-&gt;target(i), total);\n}\n</code></pre> <p>An example of a callback function that dequeues values from its neighbors and averages them is below. Note that dequeueing copies data from the message queues of source blocks; the data have already been sent.</p> <pre><code>// dequeue values received from neighbors and average them\nvoid average_neighbors(Block* b,                             // local block\nconst diy::Master::ProxyWithLink&amp; cp) // communication proxy for neighbor blocks\n{\ndiy::Link*    l = cp.link();\n// for all neighbor blocks, dequeue data received from this neighbor block in the last exchange\nint total = 0;\nfor (int i = 0; i &lt; l.size(); ++i)\n{\nint v;\ncp.dequeue(l-&gt;target(i).gid, v);\ntotal += v;\n}\n// compute average\nb-&gt;average = float(total) / l.size();\n}\n</code></pre> <p>In between calling the above two functions, data are typically exchanged by <code>diy:Master</code> in the following pattern:</p> <pre><code>    ...\nmaster.foreach(&amp;local_sum);                         // compute sum of local values and enqueue to neighbors\nmaster.exchange();                                  // exchange enqueued data between neighbor blocks\nmaster.foreach(&amp;average_neighbors);                 // dequeue values received from neighbors and average them\n...\n</code></pre> <p>Callback functions can always have additional user-defined arguments following the mandatory two arguments shown above. Simply define a lambda function when calling <code>master.foreach</code> as follows.</p> <pre><code>void foo(Block* b,                             // local block\nconst diy::Master::ProxyWithLink&amp; cp, // communication proxy for neighbor blocks\nint extra_arg);                       // user-defined additional argument(s)\nint main(int argc, char** argv)\n{\n...\nint extra_arg = ...;\nmaster.foreach([&amp;](Block* b, const diy::Master::ProxyWithLink&amp; cp)\n{ foo(b, cp, extra_arg); });\n...\n}\n</code></pre> <p>If the callback is a member function of <code>Block</code>, then it can be called directly through the block without passing a pointer to the block as an argument:</p> <pre><code>struct Block\n{\n...\nvoid bar(const diy::Master::ProxyWithLink&amp; cp, // communication proxy for neighbor blocks\nint extra_arg)                        // user-defined additional argument(s)\n{\n...\n}\n};\nint main(int argc, char** argv)\n{\n...\nint extra_arg = ...;\nmaster.foreach([&amp;](Block* b, const diy::Master::ProxyWithLink&amp; cp)\n{ b-&gt;bar(cp, extra_arg); });\n...\n}\n</code></pre>"},{"location":"decomposition/","title":"Domain decomposition","text":"<p>The <code>diy::RegularDecomposer</code> is used to decompose a domain into a regular grid of blocks. The blocks are created, linked by nearest neighbors, and added to the master inside of a user-defined callback function that is passed to the decomposer. The domain and resulting block bounds can be either discrete (integer-valued) or continuous (floating-point).</p> <p>Not all problems have regular domains and regular block decompositions. <code>Decomposer</code> is a helper object for regular problems, but its use is optional. For irregular (e.g., graph) problems, the domain can be custom decomposed by the user and blocks can be linked to neighbors manually.</p>"},{"location":"decomposition/#callback-function-for-decomposer","title":"Callback function for Decomposer","text":"<p>A callback function passed to the decomposer creates blocks, links them together by nearest neighbors, and adds them to the <code>Master</code> object. The signature for such a function and an example of its contents are below. This function can be defined globally or as a lambda function. Two sets of block bounds, <code>core</code> and <code>bounds</code> are provided to the user: the former exclude any ghost region (overlap between blocks) while the latter include any ghost region. In addition, the overall global domain bounds are provided as a convenience in <code>domain</code>. We recommended that the user store some of these bounds in the block for further reference, as they are the result of the decomposition and usually extremely useful.</p> <pre><code>typedef     diy::RegularGridLink        RGLink;\nvoid link(int gid,\nconst Bounds&amp; core,                   // the block bounds excluding any ghost region\nconst Bounds&amp; bounds,                 // the block bounds including any ghost region\nconst Bounds&amp; domain,                 // the global domain bounds\nconst RGLink&amp; link)                   // link to the neighboring blocks\n{\nBlock*          b   = new Block;             // create a new block, perform any custom initialization\nRGLink*         l   = new RGLink(link);      // copy the link so that master owns a copy\nint             lid = master.add(gid, b, l); // add block to the master (mandatory)\n// process any additional args here, save the bounds, etc.\n}\n</code></pre>"},{"location":"decomposition/#constructing-and-using-decomposer","title":"Constructing and using Decomposer","text":"<p>Various ways to construct and use the Decomposer are illustrated in the snippet below.</p> <pre><code>typedef     diy::RegularContinuousLink        RCLink;\ndiy::Master master(...);\n// share_face is a vector of bools indicating whether faces are shared in each dimension\n// uninitialized values default to false\ndiy::RegularDecomposer&lt;Bounds&gt;::BoolVector          share_face;\n// wrap is a vector of bools indicating whether boundary conditions are periodic in each dimension\n// uninitialized values default to false\ndiy::RegularDecomposer&lt;Bounds&gt;::BoolVector          wrap;\n// ghosts is a vector of ints indicating number of ghost cells per side in each dimension\n// uninitialized values default to 0\ndiy::RegularDecomposer&lt;Bounds&gt;::CoordinateVector    ghosts;\n// --- various ways to decompose a domain follow ---\n// create a RegularDecomposer\ndiy::RegularDecomposer&lt;Bounds&gt; decomposer(dim,              // dimensionality of the domain\ndomain,           // overall bounds of global domain\nnblocks,          // global number of blocks\nshare_face,       // optional\nwrap,             // optional\nghosts);          // optional\n// --- and ---\n// call the decomposer's decompose function given a callback function of the signature above\ndecomposer.decompose(world.rank(),                  // MPI rank of this process\nassigner,                      // diy::Assigner object\nlink);                         // callback function for decomposer\n// --- or ---\n// call the decomposer's decompose function given a lambda\ndecomposer.decompose(world.rank(),                  // MPI rank of this process\nassigner,                      // diy::Assigner object\n[&amp;](int gid,                   // block global id\nconst Bounds&amp; core,        // block bounds without any ghost added\nconst Bounds&amp; bounds,      // block bounds including any ghost region added\nconst Bounds&amp; domain,      // global data bounds\nconst RCLink&amp; link)        // link to neighboring blocks\n{\nBlock*          b   = new Block;             // create a new block, perform any custom initialization\nRGLink*         l   = new RCLink(link);      // copy the link provided so that master owns a copy\nint             lid = master.add(gid, b, l); // add block to the master (mandatory)\n// process any additional args here, save the bounds, etc.\n});\n// --- or ---\n// call the decomposer's decompose function given master only\n// (add the block to master and do nothing else)\ndecomposer.decompose(world.rank(),\nassigner,\nmaster);\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>The best way to learn DIY is by example, and the examples directory contains numerous complete programs that demonstrate most of the concepts in DIY. Example names preceded by the heading (commented) have ample comments embedded in the source code, meaning they are good starting points. The remaining, uncommented examples are easier to understand once the commented ones are clear.</p> <ul> <li> <p>Simple:   includes common operations such as initializing DIY, adding links to form block   neighborhoods, creating callback functions for each block, communicating   between block neighbors, performing collectives over all blocks, and writing   and reading blocks to and from a DIY file. These examples also show how to   create, destroy, and serialize blocks in (commented) block.h.</p> <ul> <li> <p>(commented) simple.cpp: exercises simple neighbor communication by creating a linear chain of blocks, each connected to two neighbors (predecessor and successor), except for the first and the last blocks, which have only one or the other. Each block computes an average of its values and those of its neighbors. The average is stored in the block, and the blocks are written to a file in storage.</p> </li> <li> <p>until-done.cpp: shows how to use collectives in the DIY model, i.e., block-based collectives that can run in-/out-of-core and single/multithreaded. The same result can be accomplished using the reduction patterns in examples/reduce, but using DIY collectives can be easier for simple \"one-line\" reductions such as global sums or boolean ands. This example shows how to do an all_reduce() in order to determine whether all blocks have finished processing.</p> </li> <li> <p>read-blocks.cpp: shows how to read a DIY file back into memory and create a new DIY master and assignment of blocks to processes that may be different than when the blocks were written. The blocks from simple.cpp are read back into memory from the file, and their values are printed.</p> </li> </ul> </li> <li> <p>Decomposition:   demonstrates how to decompose a regular grid into blocks and create links between them.</p> <ul> <li> <p>(commented) regular-decomposer-long.cpp: blocks are defined via a lambda function provided to the decomposer. This example shows how to set shared faces, ghost regions, and periodic boundaries in the decomposition. It also shows both how to create a <code>RegularDecomposer</code> and call its <code>decompose</code> member function, as well as how to combine those two steps using one helper function.</p> </li> <li> <p>(commented) regular-decomposer-short.cpp: domain can be decomposed just by providing a <code>Master</code> object to the decomposer.</p> </li> </ul> </li> <li> <p>Serialization:   blocks that are loaded and saved in and out of core are serialized by DIY. This   example shows how to write the <code>load</code> and <code>save</code> functions for two data   structures. Both require just a one-line definition because DIY can serialize   such structures automatically.</p> <ul> <li>test-serialization.cpp: a 3-d <code>Point</code> and an n-d <code>PointVec</code> are serialized automatically, and the main program tests loading and saving both structures.</li> </ul> </li> <li> <p>MPI:   shows DIY's convenience wrapper for MPI. It exists only to make the   code simpler; the user is free to use it or the original MPI routines   interchangeably.</p> <ul> <li>test-mpi.cpp: this example exercises <code>send</code>, <code>receive</code>, <code>iprobe</code>, <code>broadcast</code>, <code>reduce</code>, <code>scan</code>, and <code>all_gather</code>.</li> </ul> </li> <li> <p>I/O: illustrates   BOV (brick of values) and NumPy I/O, both built on top of MPI-IO and MPI   sub-array types.</p> <ul> <li>test-io.cpp: tests the readers and writers for BOV and NumPy. The reader would typically be called from the <code>create</code> callback passed to diy::decompose and tell the reader to read a specific block of data that the decomposer chose. Blocks can read data collectively if the number of blocks on each processor is the same. Same goes for the writers. The writer would typically be called from the <code>foreach</code> callback for each block.</li> </ul> </li> <li> <p>Reduction:   DIY supports general reductions to implement more complex global operations   than the collective one-liners. These reductions are general-purpose (any   global communication pattern can be implemented); they operate over blocks,   which cycle in and out of core as necessary; the   operations are (multi-)threaded automatically using the same mechanism as   <code>diy::Master::foreach()</code>. Although any global communication can   be expressed using the reduction mechanism, all the reductions included in DIY   operate in rounds over a k-ary reduction tree. The value of k used in each round   can vary, but if it's fixed, the number of rounds is log_k(nblocks).</p> <ul> <li> <p>(commented) merge-reduce.cpp: merges blocks together, computing a sum of their values. At each round, one block of a group of k blocks is the root of the group. The other blocks send their data to the root, which computes the sum, and the root block (only) proceeds to the next round. After <code>log_k(numblocks)</code> rounds, one block contains the global sum of the values. Calling merge-reduce is done by creating <code>diy::RegularMergePartners</code> and then calling <code>diy::reduce</code>. For regular grids of blocks, groups can be formed by either \"distance-doubling\" or \"distance-halving\" depending on the value of the <code>contiguous</code> parameter in <code>diy::RegularMergePartners</code>.</p> </li> <li> <p>(commented) swap-reduce.cpp: unlike merge-reduce, the swap-reduction does not idle blocks from one round to the next and does not aggregate all the results to a single block. Rather, block data are split into k pieces that are swapped between the k members of a group. This particular example begins with an unsorted set of points that do not lie in the bounds of the blocks, and the swap reduction is used to sort the points in the correct blocks with respect to the block bounds.</p> </li> <li> <p>all-to-all.cpp: this example solves the same problem as swap-reduce.cpp: sorts points in blocks. The difference is that swap-reduce.cpp does everything manually; whereas in all-to-all.cpp, the user only specifies how to enqueue data (from each block to each block) at the start and how to dequeue data at the end. <code>diy::all_to_all()</code> takes care of all the intermediate rounds, routing the data appropriately.</p> </li> <li> <p>(commented) all-done.cpp: this is another example of using <code>diy::all_to_all()</code> for a very common case: to determine globally whether any blocks have any local work left to do.</p> </li> <li> <p>sort.cpp: shows how to use reduction to sort a 1-d vector of integers. The algorithm is a histogram-based sort that combines both merge and swap reductions. It merges histograms of local data distributions, computes quantiles of the histograms, and then swaps data values among blocks based on the quantiles.</p> </li> </ul> </li> <li> <p>Algorithms:   Examples of the following algorithms are included.</p> <ul> <li> <p>sample-sort.cpp: This example calls the parallel sample sort algorithm of [Blelloch 1998] that's included in DIY. The built-in  <code>diy::sort</code> used in this example is easier to use than the manual sort implemented by the example above, and we are in the process of comparing the performance of the two versions. Note that sort.cpp can only sort arithmetic types (to be able to compute the histograms), whereas <code>diy::sort</code> supports any type, which has a user-supplied comparison operation.</p> </li> <li> <p>kd-tree.cpp: Like the swap-reduce example above, this example begins with an unsorted set of points that do not lie in the bounds of any blocks, but the points are sorted into kd-tree of blocks with approximately equal numbers of points in each block.</p> </li> </ul> </li> </ul> <p>Various other open-source projects have been DIY'ed, and these are also good, albeit more involved, places to learn DIY. Here are a few suggestions:</p> <ul> <li>cian2 is a suite of benchmarks that test   various HPC tasks. The communication part of cian   tests common communication patterns including most of the above reductions and   neighbor communication.</li> <li>tess2 is a parallel Voronoi and Delaunay tessellation library that is parallelized using DIY.<ul> <li>The src directory is the library code that uses DIY to compute the tessellation in parallel and write it to disk in the DIY block format.</li> <li>The examples directory contains numerous DIY programs that use the tess library.</li> <li>The tools directory contains a serial rendering program <code>draw.cpp</code> that reads the DIY block format from disk and uses it to draw the tessellation.</li> </ul> </li> </ul>"},{"location":"initialization/","title":"Initialization","text":"<p>Most DIY programs follow a similar sequence of initial steps to setup DIY before the actual parallel computing happens. These steps include:</p> <ul> <li>Defining a block data structure</li> <li>Creating a <code>diy::Master</code> object to manage the blocks local to one MPI process</li> <li>Assigning the local blocks to the master with a <code>diy:Assigner</code> object</li> <li>Decomposing the global domain into blocks with the <code>diy::Decomposer</code> object</li> </ul> <p>The following is an example of the steps needed to initialize DIY.</p>"},{"location":"initialization/#block","title":"Block","text":"<p>The block is the basic unit of everything (data, decomposition, communication) in DIY. Use it to define your data model and any state associated with the data that will be needed to accompany the data throughout its lifetime. In addition to the data in the block, you must define functions to <code>create</code> and <code>destroy</code> the block that DIY can call. (Technically, if you choose to manage block memory yourself and don't pass <code>create</code> and <code>destroy</code> functions to <code>diy::Master</code> below, then you don't need to define <code>create</code> and <code>destroy</code> here. However, we recommend having <code>Master</code> manage block creation and destruction to prevent possible leaks.) If the blocks are intended to be moved in and out of core, then the block must also define <code>save</code> and <code>load</code> functions. (<code>create</code>, <code>destroy</code>, and optionally <code>save</code> and <code>load</code> could also be defined globally outside of the block, if you wish.)</p> <pre><code>struct Block\n{\nstatic void*    create()                                    { return new Block; }\nstatic void     destroy(void* b)                            { delete static_cast&lt;Block*&gt;(b); }\nstatic void     save(const void* b, diy::BinaryBuffer&amp; bb)  { diy::save(bb, *static_cast&lt;const Block*&gt;(b)); }\nstatic void     load(void* b, diy::BinaryBuffer&amp; bb)        { diy::load(bb, *static_cast&lt;Block*&gt;(b)); }\n// your user-defined member functions\n...\n// your user-defined data members\n...\n}\n</code></pre>"},{"location":"initialization/#master","title":"Master","text":"<p>diy::Master owns and manages the blocks that are assigned to the current MPI process. To set up a <code>Master</code> object, first define the MPI communicator and the file storage object (if blocks will be moved in and out of core), which you pass to the <code>Master</code> constructor. <code>Master</code> manages loading/saving blocks, executing their callback functions, and exchanging data between them.</p> <pre><code>int main(int argc, char** argv)\n{\n...\ndiy::mpi::communicator    world(comm);\ndiy::FileStorage          storage(\"./DIY.XXXXXX\");\ndiy::Master               master(world,           // MPI world communicator\nnum_threads,     // # blocks to execute concurrently (1 means sequence through local blocks)\nmem_blocks,      // # blocks to keep in memory (-1 means keep all blocks in memory)\n&amp;Block::create,  // block create function\n&amp;Block::destroy, // block destroy function\n&amp;storage,        // storage location for out-of-core blocks (optional)\n&amp;Block::save,    // block save function for out-of-core blocks (optional)\n&amp;Block::load);   // block load function for out-of-core blocks (otpional)\n...\n}\n</code></pre> <p>If all blocks are to remain in memory, there is no need to specify <code>storage</code>, <code>save</code>, and <code>load</code>, which are for block loading/unloading.</p> <pre><code>  diy::mpi::communicator    world(comm);\ndiy::Master               master(world,            // MPI world communicator\nnum_threads,      // # threads DIY can use to execute mulitple local blocks concurrently (optional, default 1)\nmem_blocks,       // # blocks to keep in memory, -1 means all blocks (optional)\n&amp;Block::create,   // block create function (optional)\n&amp;Block::destroy); // block destroy function (optional)\n</code></pre>"},{"location":"initialization/#assigner","title":"Assigner","text":"<p>diy::Assigner is an auxiliary object that determines what blocks lives on what MPI process. Blocks can be assigned to processes contiguously or in round-robin fashion:</p> <pre><code>diy::ContiguousAssigner   assigner(world.size(),   // total number of MPI ranks\nnblocks);       // total number of blocks in global domain\n// --- or ---\ndiy::RoundRobinAssigner   assigner(world.size(),\nnblocks);\n</code></pre>"},{"location":"initialization/#decomposer","title":"Decomposer","text":"<p>Any custom decomposition can be formed by creating blocks yourself and linking them together into communicating neighborhoods manually. However, for a regular grid of blocks, DIY provides a regular decomposition of blocks with either continuous (floating-point extents that share common boundaries) or discrete (integer extents that may or may not overlap) bounds.</p> <pre><code>typedef  diy::ContinuousBounds       Bounds;\ntypedef  diy::RegularContinuousLink  RCLink;\n// --- or ---\ntypedef  diy::DiscreteBounds         Bounds;\ntypedef  diy::RegularGridLink        RGLink;\n</code></pre> <p><code>RegularDecomposer</code> helps decompose a domain into a regular grid of blocks. It's initialized with the dimension of the domain, its extents, and the number of blocks used in the decomposition.</p> <pre><code>diy::RegularDecomposer&lt;Bounds&gt; decomposer(dim,      // dimensionality of global domain\ndomain,   // sizes of global domain\nnblocks); // global number of blocks\n</code></pre> <p>Its member function <code>decompose</code> performs the actual decomposition. Besides the local MPI rank and an instance of <code>Assigner</code>, it takes a callback responsible for creating the block and adding it to a <code>Master</code>. In C++11, it's convenient to use a lambda function for this purpose.</p> <pre><code>decomposer.decompose(rank,                          // MPI rank of this process\nassigner,                      // diy::Assigner object\n[&amp;](int gid,                   // block global id\nconst Bounds&amp; core,        // block bounds without any ghost added\nconst Bounds&amp; bounds,      // block bounds including any ghost region added\nconst Bounds&amp; domain,      // global data bounds\nconst RCLink&amp; link)        // neighborhood\n{\nBlock*          b   = new Block;             // possibly use custom initialization\nRCLink*         l   = new RCLink(link);      // link neighboring blocks\nint             lid = master.add(gid, b, l); // add block to the master (mandatory)\n// process any additional args here, load the data, etc.\n});\n</code></pre> <p>A shorter form is provided, if you only want to add the blocks to <code>Master</code>, without any additional processing.</p> <pre><code>decomposer.decompose(rank,          // MPI rank of this process\nassigner,      // diy::Assigner object\nmaster);       // diy::Master object\n</code></pre>"},{"location":"initialization/#a-complete-example","title":"A complete example","text":"<p>Here is one version of each of the above options combined into a complete program, using a lambda function for the decomposer callback.</p> <pre><code>#include &lt;diy/decomposition.hpp&gt;\n#include &lt;diy/assigner.hpp&gt;\n#include &lt;diy/master.hpp&gt;\ntypedef     diy::DiscreteBounds       Bounds;\ntypedef     diy::RegularGridLink      RGLink;\nstruct Block\n{\nBlock() {}\nstatic void*    create()            { return new Block; }\nstatic void     destroy(void* b)    { delete static_cast&lt;Block*&gt;(b); }\n};\nint main(int argc, char* argv[])\n{\ndiy::mpi::environment     env(argc, argv);             // diy's version of MPI_Init\ndiy::mpi::communicator    world;                       // diy's version of MPI communicator\nint                       dim     = 3;                 // dimensionality\nint                       nprocs  = 8;                 // total number of MPI ranks\nint                       nblocks = 32;                // total number of blocks in global domain\ndiy::ContiguousAssigner   assigner(nprocs, nblocks);   // assign blocks to MPI ranks\nBounds domain;                                         // global data size\ndiy::Master               master(world,                // communicator\n1,                    // 1 thread in this example\n-1,                   // all blocks in memory in this example\n&amp;Block::create,       // block create function\n&amp;Block::destroy);     // block destroy function\ndiy::RegularDecomposer&lt;Bounds&gt; decomposer(dim,         // dimensionality of domain\ndomain,      // global domain size\nnblocks);    // global number of blocks\ndecomposer.decompose(world.rank(),                     // MPI rank of this process\nassigner,                         // assigner object\n[&amp;](int gid,                      // block global id\nconst Bounds&amp; core,           // block bounds without any ghost added\nconst Bounds&amp; bounds,         // block bounds including any ghost region added\nconst Bounds&amp; domain,         // global data bounds\nconst RGLink&amp; link)           // neighborhood\n{\nBlock*  b   = new Block;             // create a new block, perform any custom initialization\nRGLink* l   = new RGLink(link);      // copy the link so that master owns a copy\nint     lid = master.add(gid, b, l); // add block to the master (mandatory)\n});\n}\n</code></pre>"},{"location":"io/","title":"Block I/O","text":"<p>DIY writes and reads blocks to and from storage in either collective or independent mode. The collective mode, which is the default, creates one file. Independent mode creates one file per process. These features should not be confused with the temporary block I/O that DIY does in the course of shuffling blocks in and out of core while executing the block <code>foreach</code> functions. This happens behind the scenes and is not the subject of the I/O module.</p>"},{"location":"io/#collective-io","title":"Collective I/O","text":"<p>The following functions are used to write and read blocks in parallel using MPI-IO.</p> <ul> <li><code>diy::io::write_blocks()</code></li> <li><code>diy::io::read_blocks()</code></li> </ul>"},{"location":"io/#independent-io","title":"Independent I/O","text":"<p>The following functions are used to write and read blocks serially using Posix.</p> <ul> <li><code>diy::io::split::write_blocks()</code></li> <li><code>diy::io::split::read_blocks()</code></li> </ul>"},{"location":"io/#example","title":"Example","text":"<p>An example (of collective I/O) is below:</p> <pre><code>int mem_blocks = -1;\nint num_threads = 1;\ndiy::mpi::communicator    world(comm);\ndiy::Master               master(world,\nnum_threads,\nmem_blocks);\n// --- write ---\n// the total number of blocks, tot_blocks, is set by user\ndiy::ContiguousAssigner assigner(world.size(), tot_blocks);\n// the block_save function is unnecessary if it was defined as part of master\ndiy::io::write_blocks(filename, world, master, &amp;block_save);\n// --- read ---\n// the number of blocks is -1 because it is determined by read_blocks()\ndiy::ContiguousAssigner assigner(world.size(), -1);\n// the block_load function is unnecessary if it was defined as part of master\ndiy::io::read_blocks(filename, world, *assigner, master, &amp;block_load);\n</code></pre> <p>In addition, the examples/io directory illustrates BOV (brick of values) and NumPy I/O, which is built on top of MPI-IO and MPI sub-array types. The test-io.cpp example tests the readers and writers for BOV and NumPy.</p>"},{"location":"local_comm/","title":"Local synchronous communication","text":"<p>DIY supports synchronous local communication among neighboring blocks using <code>diy::Master::exchange()</code>.  Additional lightweight collectives can also be executed over the communication proxy of the synchronous <code>exchange</code> mechanism.</p>"},{"location":"local_comm/#neighboring-blocks-connected-by-diymasterproxywithlink","title":"Neighboring blocks connected by <code>diy::Master::ProxyWithLink</code>","text":"<p>In all of the following communication patterns, local means that a block communicates only with its \"neighboring\" blocks. The neighborhood is defined as the collection of edges in a communication graph from the current block to some other blocks; information is exchanged over these edges. In DIY, the communication proxy encapsulates the local communicator over the neighborhood. The communication proxy includes the link, which is the collection of edges to the neighboring blocks. Combined, the communication proxy and the link are contained in <code>diy::Master::ProxyWithLink</code>.  This size of the link is the number of communication graph edges (i.e., neighboring blocks), and <code>link-&gt;target[i]</code> is the block connected to the i-th edge. Edges can be added or removed dynamically. Data can be enqueued to and dequeued from any target in the link as desired.</p>"},{"location":"local_comm/#synchronous-exchange","title":"Synchronous <code>exchange</code>","text":"<p>The synchronous <code>exchange</code> protocol has three steps:</p> <ul> <li>a callback function enqueues data to some of the blocks in its link (perhaps after peforming some local computation)</li> <li><code>diy::Master::exchange</code> synchronously exchanges message queues</li> <li>another callback function dequeues the received data from its link (and perhaps performs some local computation)</li> </ul> <pre><code>void foo(Block* b,                             // local block\nconst diy::Master::ProxyWithLink&amp; cp) // communication proxy for neighbor blocks\n{\ndiy::Link*    l = cp.link();                     // link to the neighbor blocks\n// compute some local value\n...\n// for all neighbor blocks, enqueue data going to this neighbor block in the next exchange\nfor (int i = 0; i &lt; l-&gt;size(); ++i)\ncp.enqueue(l-&gt;target(i), value);\n}\nvoid bar(Block* b,                             // local block\nconst diy::Master::ProxyWithLink&amp; cp) // communication proxy for neighbor blocks\n{\ndiy::Link*    l = cp.link();\n// for all neighbor blocks, dequeue data received from this neighbor block in the last exchange\nfor (int i = 0; i &lt; l.size(); ++i)\n{\nint v;\ncp.dequeue(l-&gt;target(i).gid, v);\n}\n// compute some local value\n...\n}\nint main(int argc, char**argv)\n{\n...\nmaster.foreach(&amp;foo);\nmaster.exchange();\nmaster.foreach(&amp;bar);\n}\n</code></pre> <p>Recall that <code>master.foreach()</code> executes the functions <code>foo</code> and <code>bar</code> on all of the blocks in the current MPI process, so all blocks have the opportunity to enqueue and dequeue data to/from their neighbors. Because <code>master.exchange</code> is called explicitly, it synchronizes all of the blocks in between calling <code>foo</code> and <code>bar</code>, effectively acting as a barrier.</p>"},{"location":"local_comm/#proxy-collectives-for-exchange","title":"Proxy collectives for <code>exchange</code>","text":"<p>When a lightweight reduction needs to be mixed into an existing pattern such as a neighborhood exchange, DIY has a mechanism for this. An example is a neighbor exchange that must iterate until the collective result indicates it is time to terminate (as in particle tracing in rounds until no block has any more work to do). The underlying mechanism works as follows.</p> <ul> <li>The input values from each block are pushed to a vector of inputs for the process.</li> <li>The corresponding MPI collective is called by the process.</li> <li>The reduced results are redistributed to the process' blocks.</li> </ul> <p>The inputs are pushed by calling <code>all_reduce</code> from each block, and the outputs are popped by calling <code>get</code> from each block. The collective mechanism has the following characteristics.</p> <ul> <li>It is a simple member function of the communication proxy that is piggybacked onto the underlying proxy, not a completely new pattern.</li> <li>The reduction works per block and can be out of core because it uses the underlying proxy.</li> <li>The operator is a simple predefined macro. The following operators are available: <code>maximum&lt;T&gt;</code>, <code>minimum&lt;T&gt;</code>,   <code>std::plus&lt;T&gt;</code>, <code>std::multiplies&lt;T&gt;</code>, <code>std::logical_and&lt;T&gt;</code>, and <code>std::logical_or&lt;T&gt;</code>.</li> <li>The result is retrieved using the <code>get</code> function, not dequeue.</li> <li>Currently, all_reduce is the only collective available.</li> </ul> <p>A code snippet is below. The complete example is here</p> <pre><code>void foo(Block* b,                             // local block\nconst diy::Master::ProxyWithLink&amp; cp) // communication proxy\n{\n...\ncp.all_reduce(value, std::plus&lt;int&gt;());      // local value being reduced\n}\nvoid bar(Block* b,                             // local block\nconst diy::Master::ProxyWithLink&amp; cp) // communication proxy\n{\nint total = cp.get&lt;int&gt;();                   // result of the reduction\n...\n}\nint main(int argc, char**argv)\n{\n...\nmaster.foreach(&amp;foo);\nmaster.exchange();\nmaster.foreach(&amp;bar);\n}\n</code></pre>"},{"location":"mpi/","title":"MPI convenience wrapper","text":"<p>The block-parallel model in DIY passes messages between blocks rather than processes, as MPI does. There are many advantages to block parallelism as opposed to process parallelism---flexible assignment of blocks to processes, in/out-of-core block and message queue migration, block threading, and others---but sometimes you may still want to perform some operation using process parallelism and send plain old MPI messages between processes instead of blocks (even though we highly encourage you to think and write in terms of block parallelism and avoid the temptation to revert to process parallelism).</p> <p>It's possible to mix and match the two paradigms, and nothing prevents inserting plain old MPI calls in a DIY program in addition to using DIY's block-parallel features. In such cases, one can use MPI's syntax, but we recommend using DIY's wrapper around MPI instead.</p> <p>DIY includes a simple C++ wrapper around the MPI commands. It borrows heavily from Boost.MPI design.</p> <pre><code>#include &lt;diy/mpi.hpp&gt;\nint main(int argc, char* argv[])\n{\ndiy::mpi::environment     env(argc, argv);        // RAII\ndiy::mpi::communicator    world;\n...\n}\n</code></pre> <p>A simple wrapper <code>diy::mpi::environment</code> takes care of MPI initialization and finalization using the RAII (resource acquisition is initialization) idiom. <code>diy::mpi::communicator</code> wraps an <code>MPI_Comm</code> object and provides methods to <code>send()</code> and <code>recv()</code> as well as <code>isend()</code> and <code>irecv()</code>.</p> <p>All functions map C++ types to the appropriate MPI datatypes, with a specialized interface for an <code>std::vector</code> of them.</p>"},{"location":"mpi/#communicator","title":"Communicator","text":"<p>The following functions are members of <code>diy::mpi::communicator</code>.</p> <ul> <li>rank and size: <code>diy::mpi::rank()</code>, <code>diy::mpi::size()</code></li> <li>point-to-point communication: <code>diy::mpi::send()</code>, <code>diy::mpi::recv()</code>, <code>diy::mpi::isend()</code>, <code>diy::mpi::issend()</code>, <code>diy::mpi::irecv()</code></li> <li>probe: <code>diy::mpi::probe()</code>, <code>diy::mpi::iprobe()</code></li> <li>barrier: <code>diy::mpi::barrier()</code>, <code>diy::mpi::ibarrier()</code></li> <li>derive new communicator: <code>diy::mpi::split()</code>, <code>diy::mpi::duplicate()</code></li> </ul>"},{"location":"mpi/#point-to-point-communication","title":"Point-to-point communication","text":"<p>In addition to the above member functions of <code>diy::mpi::communicator</code>, the same point-to-point functions are available if passing the <code>diy::mpi::communicator</code> as an argument.</p> <ul> <li><code>diy::mpi::send()</code>, <code>diy::mpi::recv()</code>, <code>diy::mpi::isend()</code>, <code>diy::mpi::issend()</code>, <code>diy::mpi::irecv()</code></li> </ul>"},{"location":"mpi/#collectives","title":"Collectives","text":"<ul> <li><code>diy::mpi::broadcast()</code>, <code>diy::mpi::ibroadcast()</code></li> <li><code>diy::mpi::gather()</code>, <code>diy::mpi::gather_v()</code>, <code>diy::mpi::all_gather()</code>, <code>diy::mpi::all_gather_v()</code>,</li> <li><code>diy::mpi::reduce()</code>, <code>diy::mpi::all_reduce()</code>, <code>diy::mpi::iall_reduce()</code></li> </ul> <pre><code>   int in = comm.rank() * 3;\nif (comm.rank() == 0)\n{\nint out;\nmpi::reduce(comm, in, out, std::plus&lt;float&gt;());\n} else\nmpi::reduce(comm, in, std::plus&lt;float&gt;());\n</code></pre> <ul> <li><code>diy::mpi::scan()</code>, <code>diy::mpi::all_to_all()</code></li> </ul>"},{"location":"mpi/#operations","title":"Operations","text":"class MPI operation <code>diy::mpi::maximum&lt;U&gt;</code> <code>MPI_MAX</code> <code>diy::mpi::minimum&lt;U&gt;</code> <code>MPI_MIN</code> <code>std::plus&lt;U&gt;</code> <code>MPI_SUM</code> <code>std::multiplies&lt;U&gt;</code> <code>MPI_PROD</code> <code>std::logical_and&lt;U&gt;</code> <code>MPI_LAND</code> <code>std::logical_or&lt;U&gt;</code> <code>MPI_LOR</code>"},{"location":"mpi/#others","title":"Others","text":"<p>There are several other classes of MPI functions wrapped in DIY, including:</p> <ul> <li>status and request for nonblocking communication</li> <li>I/O</li> <li>one-sided windows</li> </ul> <p>Please refer to the DIY header files for details.</p>"},{"location":"ooc/","title":"Writing out-of-core algorithms","text":"<p>Typically, out-of-core algorithms are distinct from their in-core counterparts, and much research has been conducted on out-of-core algorithms for specific classes of problems. In DIY, switching from in-core to out-of-core is as simple as changing the number of blocks allowed to reside in memory. If this is a command-line argument, no recompilation is needed. This is another advantage of the block parallel programming model. When a program is written in terms of logical blocks, the DIY runtime is free to migrate blocks and their associated message queues in and out-of-core, with no change to the program logic.</p>"},{"location":"ooc/#block","title":"Block","text":"<p>Recall from the Initialization module that if blocks are intended to be moved in and out-of-core, then the block must define <code>save</code> and <code>load</code> functions in addition to <code>create</code> and <code>destroy</code> (<code>create</code>, <code>destroy</code>, <code>save</code>, and <code>load</code> could also be defined globally outside of the block, if you wish).</p> <pre><code>struct Block\n{\nstatic void*    create()                                    { return new Block; }\nstatic void     destroy(void* b)                            { delete static_cast&lt;Block*&gt;(b); }\nstatic void     save(const void* b, diy::BinaryBuffer&amp; bb)  { diy::save(bb, *static_cast&lt;const Block*&gt;(b)); }\nstatic void     load(void* b, diy::BinaryBuffer&amp; bb)        { diy::load(bb, *static_cast&lt;Block*&gt;(b)); }\n// your user-defined member functions\n...\n// your user-defined data members\n...\n}\n</code></pre>"},{"location":"ooc/#master","title":"Master","text":"<p>Recall from Initialization that diy::Master owns and manages the blocks that are assigned to the current MPI process. For out-of-core operation, the <code>storage</code> object and the <code>load</code> and <code>save</code> objects are mandatory. <code>Master</code> manages loading/saving blocks, executing their callback functions, and exchanging data between them including when blocks are out-of-core.</p> <p>To initiate out-of-core operation, simply change the <code>mem_blocks</code> argument to <code>Master</code> from -1 (meaning all blocks in core) to a value greater than or equal to 1. For example, assume we have a program with 32 total blocks, run on 8 MPI processes. DIY's <code>Assigner</code> will assign 32 / 8 = 4 blocks to each MPI process. If we only have sufficient memory to hold 2 blocks at a time in memory, setting <code>memblocks = 2</code> is all that is needed; DIY does the rest.</p> <p>If <code>Block</code> is defined as above, the first part of the code looks like this.</p> <pre><code>#include &lt;diy/assigner.hpp&gt;\n#include &lt;diy/master.hpp&gt;\nint main(int argc, char* argv[])\n{\ndiy::mpi::environment     env(argc, argv);             // diy's version of MPI_Init\ndiy::mpi::communicator    world;                       // diy's version of MPI communicator\ndiy::FileStorage          storage(\"./DIY.XXXXXX\");     // storage location for out-of-core blocks\nint                       nprocs      = 8;             // total number of MPI ranks\nint                       nblocks     = 32;            // total number of blocks in global domain\nint                       mem_blocks  = 2;             // number of blocks that will fit in memory\ndiy::ContiguousAssigner   assigner(nprocs, nblocks);   // assign blocks to MPI ranks\ndiy::Master               master(world,                // communicator\n1,                    // use 1 thread to execute blocks\nmem_blocks,           // # blocks in memory\n&amp;Block::create,       // block create function\n&amp;Block::destroy,      // block destroy function\n&amp;storage,             // storage location for out-of-core blocks\n&amp;Block::save,         // block save function for out-of-core blocks\n&amp;Block::load);        // block load function for out-of-core blocks\n...\n</code></pre>"},{"location":"reduce_comm/","title":"Global communication (reductions)","text":"<p>DIY supports general reductions to implement complex global operations over blocks.  These reductions are general-purpose (any global communication pattern can be implemented); they operate over blocks, which cycle in and out-of-core as necessary; the operations are (multi-)threaded automatically using the same mechanism as <code>foreach()</code>.</p> <p>All the reductions included in DIY operate in rounds over a k-ary reduction tree. The value of k used in each round can vary, but if it's fixed, the number of rounds is log_k(nblocks). In each round of the reduction tree, the distance between blocks (in terms of their block global ID or <code>gid</code>) either increases (so-called \"distance-doubling\") or decreases (so-called \"distance-halving\"). This is controlled by the <code>contiguous</code> argument, which defaults to <code>true</code> (distance-doubling). Only blocks decomposed by a <code>diy::RegularDecomposer</code> (i.e., blocks arranged in a regular block grid) can be reduced this way. The decomposer object is one of the arguments to the reduction.</p> <p>The following patterns are currently available. With the exception of <code>all_to_all</code>, the other patterns all feature a call to <code>diy::reduce</code>, passing a derived class of <code>diy::RegularPartners</code> to differentiate between the different patterns. The callback function passed to <code>diy::reduce</code> is similar to the callback function used with <code>diy::Master:foreach</code> described in the Callbacks page. The callback function is called by DIY in each round of the reduction tree. Each time, the communication proxy (<code>ReduceProxy</code>) acts similarly as in <code>foreach</code>: the proxy contains the neighboring blocks that form a communicating group for the current round. Unlike <code>foreach</code>, this group changes from one round to the next in a reduction. In addition to a pointer to the block and the communication proxy, the callback is given the partners class, which can provide additional information. The user needs to write the body of the callback function, which typically dequeues received messages from the previous round, performs some local computation, and enqeues messages for the next round.</p>"},{"location":"reduce_comm/#merge-reduce","title":"Merge-reduce","text":"<p>A merge-reduce accumulates information in successively fewer blocks with each round. Each round, one block in a communicating group (<code>ReduceProxy</code>) takes ownership of the merged result and continues to the next round, while the other blocks in the group idle in the following rounds. In the final round, one block, the root of the reduction tree, has all of the data.</p> <pre><code>void foo(Block* b,                                  // local block\nconst diy::ReduceProxy&amp; rp,                // communication proxy\nconst diy::RegularMergePartners&amp;)          // partners of the current block\n{\n// dequeue and merge\nfor (int i = 0; i &lt; rp.in_link().size(); ++i)\n{\nint nbr_gid = rp.in_link().target(i).gid;\nstd::vector&lt;int&gt;    in_vals;\nrp.dequeue(nbr_gid, in_vals);\n// merge with current block data, e.g., accumulate sum\nfor (size_t j = 0; j &lt; in_vals.size(); ++j)\n(b-&gt;data)[j] += in_vals[j];\n}\n// step 2: enqueue\nfor (int i = 0; i &lt; rp.out_link().size(); ++i)    // redundant since out_link size = 1 for merge-reduce\nrp.enqueue(rp.out_link().target(i), b-&gt;data);\n}\nint main(int argc, char* argv[])\n{\n...\nint k = 2;                                       // the radix of the k-ary reduction tree\n// partners for merge over regular block grid\ndiy::RegularMergePartners  partners(decomposer,  // diy::Decomposer object\nk,           // radix of k-ary reduction (default 2)\ncontiguous); // contiguous = true: distance doubling (default true)\n// contiguous = false: distance halving\n// reduction\ndiy::reduce(master,                              // diy::Master object\nassigner,                            // diy::Assigner object\npartners,                            // diy::RegularMergePartners object\n&amp;foo);                               // merge operator callback function\n...\n}\n</code></pre> <p>see the example here</p>"},{"location":"reduce_comm/#broadcast","title":"Broadcast","text":"<p>A broadcast is implemented as a merge-reduce in reverse order. Starting with one block, each round features more blocks participating in the communication. The usage is similar to merge-reduce, with the partners object now being `diy::RegularBroadcastPartners'.</p> <pre><code>void foo(Block* b,                                      // local block\nconst diy::ReduceProxy&amp; rp,                    // communication proxy\nconst diy::RegularBroadcastPartners&amp;);         // partners of the current block\nint main(int argc, char* argv[])\n{\n...\n// partners for broadcast over regular block grid\ndiy::RegularBroadcastPartners partners(decomposer,  // diy::Decomposer object\nk,           // radix of k-ary reduction (default 2)\ncontiguous); // contiguous = true: distance doubling (default true)\n// contiguous = false: distance halving\n// reduction\ndiy::reduce(master,                                 // diy::Master object\nassigner,                               // diy::Assigner object\npartners,                               // diy::RegularBroadcastPartners object\n&amp;foo);                                  // broadcast operator callback function\n...\n}\n</code></pre>"},{"location":"reduce_comm/#swap-reduce","title":"Swap-reduce","text":"<p>Unlike a merge-reduce, the idea of a swap-reduce is to keep all blocks busy in all rounds. Typically data are split and swapped among the neighboring blocks in each round, but the actual operations performed are up to the user to write in the callback function. DIY's role in the swap-reduce is to provide different neighbors in each round to the callback function, and to keep all blocks participating in all rounds. The usage is similar to the the prior examples, with <code>diy::RegularSwapPartners</code> being used for the partners object.</p> <pre><code>void foo(Block* b,                                  // local block\nconst diy::ReduceProxy&amp; rp,                // communication proxy\nconst diy::RegularSwapPartners&amp;);          // partners of the current block\nint main(int argc, char* argv[])\n{\n...\n// partners for swap over regular block grid\ndiy::RegularSwapPartners   partners(decomposer,  // diy::Decomposer object\nk,           // radix of k-ary reduction (default 2)\ncontiguous); // contiguous = true: distance doubling (default true)\n// contiguous = false: distance halving\n// reduction\ndiy::reduce(master,                              // diy::Master object\nassigner,                            // diy::Assigner object\npartners,                            // diy::RegularSwapPartners object\n&amp;foo);                               // swap operator callback function\n...\n}\n</code></pre> <p>see the example here</p>"},{"location":"reduce_comm/#all-reduce","title":"All-reduce","text":"<p>An all-reduce is a merge-reduce followed by a broadcast of the result from the root block to all of the blocks, so that all blocks have the same result. The same usage follows from the previous examples, only changing the partners object. Usage is similar to the previous examples, with the partners class now being <code>diy::RegularAllReducePartners</code>.</p> <pre><code>void foo(Block* b,                                      // local block\nconst diy::ReduceProxy&amp; rp,                    // communication proxy\nconst diy::RegularAllReducePartners&amp;);         // partners of the current block\nint main(int argc, char* argv[])\n{\n...\n// partners for all-reduce over regular block grid\ndiy::RegularAllReducePartners partners(decomposer,  // diy::Decomposer object\nk,           // radix of k-ary reduction (default 2)\ncontiguous); // contiguous = true: distance doubling (default true)\n// contiguous = false: distance halving\n// reduction\ndiy::reduce(master,                                 // diy::Master object\nassigner,                               // diy::Assigner object\npartners,                               // diy::RegularAllReducePartners object\n&amp;foo);                                  // all-reduce operator callback function\n...\n}\n</code></pre>"},{"location":"reduce_comm/#all-to-all","title":"All-to-all","text":"<p>The previous examples followed a common pattern. All-to-all is the exception. In an all-to-all reduction, each block sends and receives from every other block. Even though all-to-all is implemented with a reduction tree, it is called as if there were a single round of direct communication between all blocks. This means there is no partners object, and the callback function is written as if there were a single round with access to all blocks through the <code>ReduceProxy</code>. However, the radix <code>k</code> of the k-ary underlying implementation is still supplied (defaults to 2).</p> <pre><code>void foo(Block* b,                                          // local block\nconst diy::ReduceProxy&amp; rp)                        // communication proxy\n{\n// outgoing\nif (rp.in_link().size() == 0)\n{\n// enqueue data to the correct blocks\nint dest_gid = ...                                  // destination block global ID\ndiy::BlockID dest = rp.out_link().target(dest_gid); // need a full BlockID; out_link targets are ordered as gids\nrp.enqueue(dest, value);\n}\n// incoming\nelse\n{\nfor (int i = 0; i &lt; rp.in_link().size(); ++i)\n{\nint gid = rp.in_link().target(i).gid;\ndiy::MemoryBuffer&amp; incoming = rp.incoming(gid);\n// copy incoming to the block\n...\n}\n}\n}\nint main(int argc, char* argv[])\n{\n...\ndiy::all_to_all(master,                                 // diy::Master object\nassigner,                               // diy::Assigner object\n&amp;foo,                                   // all-to-all operator callback function\nk);                                     // radix of k-ary underlying implementation (default 2)\n...\n}\n</code></pre> <p>see the example here</p>"},{"location":"remote_comm/","title":"Remote communication","text":"<p>Sometimes during the course of local communication, it's necessary to send or receive messages outside of the local neighborhood. For example in particle tracing, a local algorithm that exchanges particles as they cross neighboring block boundaries, the block containing a particle's final destination may need to send summary statistics carried along by the particle (such as total number of hops) back to the originating block where the particle began its journey. For such cases, there is a remote version of <code>diy::Master::exchange()</code>.</p>"},{"location":"remote_comm/#synchronous-remote-rexchange","title":"Synchronous remote <code>rexchange</code>","text":"<p>Everything follows the synchronous <code>exchange</code> protocol of the Local communication page, except that a <code>remote</code> flag set to <code>true</code> is passed to <code>master.exchange</code>, and the <code>master.foreach()</code> callback functions can access blocks outside of their neighborhood. When enqueuing data remotely, DIY doesn't know the assignment of block global ID (<code>gid</code>) to MPI process rank; this information is only kept for the local link. Hence, the full <code>diy::BlockID</code> information (a tuple of block gid and MPI process rank) must be provided by the user. For dequeuing, nothing changes when receiving remote messages. Messages arrive over the link, and their gid is known just as if the source were local.</p> <pre><code>void foo(Block* b,                             // local block\nconst diy::Master::ProxyWithLink&amp; cp) // communication proxy for neighbor blocks\n{\ndiy::Link*    l = cp.link();                     // link to the neighbor blocks\n// compute some local value\n...\n// for all neighbor blocks, enqueue data going to this neighbor block in the next exchange\nfor (int i = 0; i &lt; l-&gt;size(); ++i)\ncp.enqueue(l-&gt;target(i), value);\n// enqueue something remote outside of the neighborhood\nint dest_gid = ...;                         // block global ID of destination\nint dest_proc = ...;                        // MPI process of destination block\ndiy::BlockID dest_block = {dest_gid, dest_proc};\ncp.enqueue(dest_block, value);\n}\nvoid bar(Block* b,                             // local block\nconst diy::Master::ProxyWithLink&amp; cp) // communication proxy for neighbor blocks\n{\ndiy::Link*    l = cp.link();\n// for all neighbor blocks, dequeue data received from this neighbor block in the last exchange\nfor (int i = 0; i &lt; l.size(); ++i)\n{\nint v;\ncp.dequeue(l-&gt;target(i).gid, v);\n}\n// compute some local value\n...\n}\nint main(int argc, char**argv)\n{\n...\nbool remote = true;\nmaster.foreach(&amp;foo);\nmaster.exchange(remote);\nmaster.foreach(&amp;bar);\n}\n</code></pre>"},{"location":"serialization/","title":"Serialization","text":"<p>The primary interface for serialization in DIY are <code>diy::save()</code> and <code>diy::load()</code>.  They call the respective functions in the <code>diy::Serialization</code> class, explained below.  Array versions of the functions optimize copying of arrays of binary data (but still do the right thing for customized types).</p> <pre><code>    #include &lt;diy/serialization.hpp&gt;\nstd::vector&lt;MyStruct&gt;   vec;\n// ...\ndiy::BinaryBuffer bb;\ndiy::save(bb, vec);\n// ...\nvec.clear();\ndiy::load(bb, vec);\n</code></pre>"},{"location":"serialization/#customizing","title":"Customizing","text":"<p>The default (unspecialized) version of <code>diy::Serialization&lt;T&gt;</code> copies <code>sizeof(T)</code> bytes from <code>&amp;x</code> to or from the <code>diy::BinaryBuffer</code> via <code>diy::BinaryBuffer::save_binary()</code> and <code>diy::BinaryBuffer::load_binary()</code>.  This works out perfectly for plain old data (e.g., simple structs), but to save a more complicated type, one has to specialize <code>diy::Serialization&lt;T&gt;</code>. (Specializations are already provided for many of the STL types, for example, <code>std::vector&lt;T&gt;</code>, <code>std::map&lt;K,V&gt;</code>, <code>std::pair&lt;T,U&gt;</code>, <code>std::valarray&lt;T&gt;</code>, <code>std::string</code>, <code>std::set&lt;T&gt;</code>, <code>std::unordered_map&lt;K,V&gt;</code>, <code>std::unordered_set&lt;T&gt;</code>, <code>std::tuple&lt;Args...&gt;</code>.) As a result, one can quickly add a specialization of one's own:</p> <pre><code>struct Point\n{\nint                 x, y;\nstd::vector&lt;int&gt;    neighbors;\n};\ntemplate&lt;&gt;\nstruct Serialization&lt;Point&gt;\n{\nstatic void save(BinaryBuffer&amp; bb, const Point&amp; p)\n{\ndiy::save(bb, x);\ndiy::save(bb, y);\ndiy::save(bb, neighbors);\n}\nstatic void load(BinaryBuffer&amp; bb, Point&amp; p)\n{\ndiy::load(bb, x);\ndiy::load(bb, y);\ndiy::load(bb, neighbors);\n}\n};\n</code></pre> <p>Note that if point had only members <code>x</code> and <code>y</code>, it would be plain old data.  So we would not need to specialize <code>diy::Serialization&lt;Point&gt;</code> because <code>Point</code> could just be copied as a binary. In general, it's better to leave <code>diy::Serialization</code> unspecialized for plain old data because then serialization of <code>std::vector&lt;...&gt;</code> of that type can be optimized (by copying the entire array at once).</p>"},{"location":"threads/","title":"Automatic block threading","text":"<p>DIY can automatically execute multiple blocks resident in memory concurrently if allowed to have access to more than one thread. Similar to automatically switching from in-core to out-of-core, DIY can thread blocks automatically with no change to the program logic, and without recompiling if the number of threads is provided as a command-line argument.</p> <p>DIY's threading is fairly coarse-grained: one thread executes an entire callback function provided to  <code>diy::Master::foreach()</code>.  Because each block is a separate object, and all state is maintained in the block, DIY's  automatic threading is inherently thread-safe. Within a block callback function, the user is also free to write  finer-grain custom threaded code, assuming that adequate harware resources exist for DIY's threads and the user's, and  that there are no conflicts between different thread libraries (DIY uses pthreads).</p> <p>DIY's threading feature can also be combined with out-of-core execution.</p>"},{"location":"threads/#block","title":"Block","text":"<p>No changes are required to the block structure described in the Initialization module .</p> <pre><code>struct Block\n{\nstatic void*    create()                                    { return new Block; }\nstatic void     destroy(void* b)                            { delete static_cast&lt;Block*&gt;(b); }\n// your user-defined member functions\n...\n// your user-defined data members\n...\n}\n</code></pre>"},{"location":"threads/#master","title":"Master","text":"<p>Recall from Initialization that diy::Master owns and manages the blocks that are assigned to the current MPI process. It also executes callback functions on blocks, which is where computations on blocks occur. To execute callback functions on multiple blocks resident in memory concurrently, simply change the <code>num_threads</code> argument to <code>Master</code> to a value greater than 1. For example, assume we have a program with 32 total blocks, run on 8 MPI processes. DIY's <code>Assigner</code> will assign 32 / 8 = 4 blocks to each MPI process. Assume all 4 blocks fit in memory. Assume we can allow DIY 2 threads for executing callback functions. In this case, rather than DIY stepping serially through the 4 blocks in a process each time a callback function is called, DIY will only make two iterations through the local blocks; in each iteration two blocks will execute their callbacks concurrently.</p> <p>If <code>Block</code> is defined as above, the first part of the code looks like this.</p> <pre><code>#include &lt;diy/assigner.hpp&gt;\n#include &lt;diy/master.hpp&gt;\nint main(int argc, char* argv[])\n{\ndiy::mpi::environment     env(argc, argv);             // diy's version of MPI_Init\ndiy::mpi::communicator    world;                       // diy's version of MPI communicator\nint                       nprocs      = 8;             // total number of MPI ranks\nint                       nblocks     = 32;            // total number of blocks in global domain\nint                       num_threads = 2;             // number of threads DIY is allowed to use\ndiy::ContiguousAssigner   assigner(nprocs, nblocks);   // assign blocks to MPI ranks\ndiy::Master               master(world,                // communicator\nnum_threads,          // number of threads DIY is allowed to use\n-1,                   // all blocks remain in memory in this example\n&amp;Block::create,       // block create function\n&amp;Block::destroy);     // block destroy function\n...\n</code></pre>"}]}